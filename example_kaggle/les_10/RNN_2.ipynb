{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c668af15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ЗАВДАННЯ 1. ПРОГНОЗ СЛІВ\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, Input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Текст, який ви хочете використовувати\n",
    "texts = \"\"\"\n",
    "Думи мої, думи мої,\n",
    "Лихо мені з вами!\n",
    "Нащо стали на папері\n",
    "Сумними рядами?..\n",
    "Чом вас вітер не розвіяв\n",
    "В степу, як пилину?\n",
    "Чом вас лихо не приспало,\n",
    "Як свою дитину?..\n",
    "\n",
    "Бо вас лихо на світ на сміх породило,\n",
    "Поливали сльози... чом не затопили,\n",
    "Не винесли в море, не розмили в полі?.\n",
    "Не питали б люде, що в мене болить,\n",
    "Не питали б, за що проклинаю долю,\n",
    "Чого нуджу світом? \"Нічого робить\", —\n",
    "Не сказали б на сміх...\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "texts = texts.replace(\"\\ufeff\", \"\")  # Прибираємо перший невидимий символ\n",
    "\n",
    "maxWordsCount = 1000\n",
    "tokenizer = Tokenizer(\n",
    "    num_words=maxWordsCount,\n",
    "    filters='!–\"—#$%&;()*+,-./:<=>?@[\\\\]^_`{|}~\\t\\n\\r«»',\n",
    "    lower=True,\n",
    "    split=\" \",\n",
    "    char_level=False,\n",
    ")\n",
    "tokenizer.fit_on_texts([texts])\n",
    "\n",
    "dist = list(tokenizer.word_counts.items())\n",
    "print(dist[:10])\n",
    "\n",
    "data = tokenizer.texts_to_sequences([texts])\n",
    "res = to_categorical(data[0], num_classes=maxWordsCount)\n",
    "print(res.shape)\n",
    "\n",
    "inp_words = 3\n",
    "n = res.shape[0] - inp_words\n",
    "\n",
    "X = np.array([res[i : i + inp_words, :] for i in range(n)])\n",
    "Y = res[inp_words:]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Input((inp_words, maxWordsCount)))\n",
    "model.add(SimpleRNN(128, activation=\"tanh\"))\n",
    "model.add(Dense(maxWordsCount, activation=\"softmax\"))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"], optimizer=\"adam\")\n",
    "\n",
    "history = model.fit(X, Y, batch_size=32, epochs=50)\n",
    "\n",
    "\n",
    "def buildPhrase(texts, str_len=20):\n",
    "    res = texts\n",
    "    data = tokenizer.texts_to_sequences([texts])[0]\n",
    "    for i in range(str_len):\n",
    "        x = to_categorical(\n",
    "            data[i : i + inp_words], num_classes=maxWordsCount\n",
    "        )  # преобразуем в One-Hot-encoding\n",
    "        inp = x.reshape(1, inp_words, maxWordsCount)\n",
    "\n",
    "        pred = model.predict(inp)\n",
    "        indx = pred.argmax(axis=1)[0]\n",
    "        data.append(indx)\n",
    "\n",
    "        res += \" \" + tokenizer.index_word[indx]  # Дописуємо рядок\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "res = buildPhrase(\"Думи мої думи\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2810969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Завдання 2. Прогнозування слів з Embedding\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, Input, Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Текст, який ви хочете використовувати\n",
    "texts = \"\"\"\n",
    "Думи мої, думи мої,\n",
    "Лихо мені з вами!\n",
    "Нащо стали на папері\n",
    "Сумними рядами?..\n",
    "Чом вас вітер не розвіяв\n",
    "В степу, як пилину?\n",
    "Чом вас лихо не приспало,\n",
    "Як свою дитину?..\n",
    "\n",
    "Бо вас лихо на світ на сміх породило,\n",
    "Поливали сльози... чом не затопили,\n",
    "Не винесли в море, не розмили в полі?.\n",
    "Не питали б люде, що в мене болить,\n",
    "Не питали б, за що проклинаю долю,\n",
    "Чого нуджу світом? \"Нічого робить\", —\n",
    "Не сказали б на сміх...\n",
    "\n",
    "\"\"\"\n",
    "texts = texts.replace(\"\\ufeff\", \"\")  # Прибираємо перший невид. символ\n",
    "\n",
    "maxWordsCount = 1000\n",
    "tokenizer = Tokenizer(\n",
    "    num_words=maxWordsCount,\n",
    "    filters='!–\"—#$%&;()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\r«»',\n",
    "    lower=True,\n",
    "    split=\" \",\n",
    "    char_level=False,\n",
    ")\n",
    "tokenizer.fit_on_texts([texts])\n",
    "\n",
    "dist = list(tokenizer.word_counts.items())\n",
    "print(dist[:10])\n",
    "\n",
    "data = tokenizer.texts_to_sequences([texts])\n",
    "\n",
    "res = np.array(data[0])\n",
    "print(res.shape)\n",
    "\n",
    "inp_words = 3\n",
    "n = res.shape[0] - inp_words\n",
    "\n",
    "X = np.array([res[i : i + inp_words] for i in range(n)])\n",
    "Y = to_categorical(res[inp_words:], num_classes=maxWordsCount)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(maxWordsCount, 256, input_length=inp_words))\n",
    "model.add(SimpleRNN(128, activation=\"tanh\"))\n",
    "model.add(Dense(maxWordsCount, activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"], optimizer=\"adam\")\n",
    "\n",
    "history = model.fit(X, Y, batch_size=32, epochs=50)\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "def buildPhrase(texts, str_len=20):\n",
    "    res = texts\n",
    "    data = tokenizer.texts_to_sequences([texts])[0]\n",
    "    for i in range(str_len):\n",
    "        x = data[i : i + inp_words]\n",
    "        inp = np.expand_dims(x, axis=0)\n",
    "\n",
    "        pred = model.predict(inp)\n",
    "        indx = pred.argmax(axis=1)[0]\n",
    "        data.append(indx)\n",
    "\n",
    "        res += \" \" + tokenizer.index_word[indx]  # дописуємо рядок\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "res = buildPhrase(\"Думи мої думи\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44da4ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Завдання 3. Побудова моделі RNN\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, Input, Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Текст, який ви хочете використовувати\n",
    "texts = \"\"\"\n",
    "Думи мої, думи мої,\n",
    "Лихо мені з вами!\n",
    "Нащо стали на папері\n",
    "Сумними рядами?..\n",
    "Чом вас вітер не розвіяв\n",
    "В степу, як пилину?\n",
    "Чом вас лихо не приспало,\n",
    "Як свою дитину?..\n",
    "\n",
    "Бо вас лихо на світ на сміх породило,\n",
    "Поливали сльози... чом не затопили,\n",
    "Не винесли в море, не розмили в полі?.\n",
    "Не питали б люде, що в мене болить,\n",
    "Не питали б, за що проклинаю долю,\n",
    "Чого нуджу світом? \"Нічого робить\", —\n",
    "Не сказали б на сміх...\n",
    "\n",
    "\"\"\"\n",
    "texts = texts.replace(\"\\ufeff\", \"\")  # Прибираємо перший невид. символ\n",
    "\n",
    "maxWordsCount = 1000\n",
    "tokenizer = Tokenizer(\n",
    "    num_words=maxWordsCount,\n",
    "    filters='!–\"—#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\r«»',\n",
    "    lower=True,\n",
    "    split=\" \",\n",
    "    char_level=False,\n",
    ")\n",
    "tokenizer.fit_on_texts([texts])\n",
    "\n",
    "dist = list(tokenizer.word_counts.items())\n",
    "print(dist[:10])\n",
    "\n",
    "data = tokenizer.texts_to_sequences([texts])\n",
    "\n",
    "res = np.array(data[0])\n",
    "print(res.shape)\n",
    "\n",
    "inp_words = 3\n",
    "n = res.shape[0] - inp_words\n",
    "\n",
    "X = np.array([res[i : i + inp_words] for i in range(n)])\n",
    "Y = to_categorical(res[inp_words:], num_classes=maxWordsCount)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(maxWordsCount, 256, input_length=inp_words))\n",
    "model.add(SimpleRNN(128, activation=\"tanh\", return_sequences=True))\n",
    "model.add(SimpleRNN(64, activation=\"tanh\"))\n",
    "model.add(Dense(maxWordsCount, activation=\"softmax\"))\n",
    "\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"], optimizer=\"adam\")\n",
    "\n",
    "history = model.fit(X, Y, batch_size=32, epochs=50)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "def buildPhrase(texts, str_len=20):\n",
    "    res = texts\n",
    "    data = tokenizer.texts_to_sequences([texts])[0]\n",
    "    for i in range(str_len):\n",
    "        x = data[i : i + inp_words]\n",
    "        inp = np.expand_dims(x, axis=0)\n",
    "\n",
    "        pred = model.predict(inp)\n",
    "        indx = pred.argmax(axis=1)[0]\n",
    "        data.append(indx)\n",
    "\n",
    "        res += \" \" + tokenizer.index_word[indx]  # дописываем строку\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "res = buildPhrase(\"Думи мої думи\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60ea9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ЗАВДАННЯ. Сентимент-Аналіз\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from tensorflow.keras.layers import Dense, LSTM, Input, Dropout, Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Задані тексти\n",
    "texts_true = [\n",
    "    \"Це текст, який містить позитивну інформацію.\",\n",
    "    \"Ще один приклад позитивного тексту.\",\n",
    "    \"Тексти можуть містити різні емоції, включаючи позитивні.\",\n",
    "]\n",
    "\n",
    "texts_false = [\n",
    "    \"Це текст, який містить негативну інформацію.\",\n",
    "    \"Не всі тексти будуть позитивними.\",\n",
    "    \"Можна зустріти і негативні тексти.\",\n",
    "]\n",
    "\n",
    "# Об'єднання текстів\n",
    "texts = texts_true + texts_false\n",
    "count_true = len(texts_true)\n",
    "count_false = len(texts_false)\n",
    "total_lines = count_true + count_false\n",
    "print(count_true, count_false, total_lines)\n",
    "\n",
    "maxWordsCount = 1000\n",
    "tokenizer = Tokenizer(\n",
    "    num_words=maxWordsCount,\n",
    "    filters='!–\"—#$%&amp;()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\r«»',\n",
    "    lower=True,\n",
    "    split=\" \",\n",
    "    char_level=False,\n",
    ")\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "dist = list(tokenizer.word_counts.items())\n",
    "print(dist[:10])\n",
    "print(texts[0][:100])\n",
    "\n",
    "max_text_len = 10\n",
    "data = tokenizer.texts_to_sequences(texts)\n",
    "data_pad = pad_sequences(data, maxlen=max_text_len)\n",
    "print(data_pad)\n",
    "\n",
    "print(list(tokenizer.word_index.items()))\n",
    "\n",
    "X = data_pad\n",
    "Y = np.array([[1, 0]] * count_true + [[0, 1]] * count_false)\n",
    "print(X.shape, Y.shape)\n",
    "\n",
    "indices = np.random.choice(X.shape[0], size=X.shape[0], replace=False)\n",
    "X = X[indices]\n",
    "Y = Y[indices]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(maxWordsCount, 128, input_length=max_text_len))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(2, activation=\"softmax\"))\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\", metrics=[\"accuracy\"], optimizer=Adam(0.0001)\n",
    ")\n",
    "\n",
    "history = model.fit(X, Y, batch_size=32, epochs=50)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "\n",
    "\n",
    "def sequence_to_text(list_of_indices):\n",
    "    words = [reverse_word_map.get(letter) for letter in list_of_indices]\n",
    "    return words\n",
    "\n",
    "\n",
    "t = \"Я люблю позитивний настрій\".lower()\n",
    "data = tokenizer.texts_to_sequences([t])\n",
    "data_pad = pad_sequences(data, maxlen=max_text_len)\n",
    "print(sequence_to_text(data[0]))\n",
    "\n",
    "res = model.predict(data_pad)\n",
    "print(res, np.argmax(res), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a885547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визначення результату\n",
    "if np.argmax(res) == 0:\n",
    "    print(\"Це негативний текст.\")\n",
    "else:\n",
    "    print(\"Це позитивний  текст.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
