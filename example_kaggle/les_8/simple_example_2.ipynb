{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Навчимо найпростішу нейронну мережу з одним нейроном (персептрон) розпізнавати логічну операцію AND.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Вхідні дані (таблиця істинності AND)\n",
    "X = np.array([[0, 0],  # 0 AND 0 = 0\n",
    "              [0, 1],  # 0 AND 1 = 0\n",
    "              [1, 0],  # 1 AND 0 = 0\n",
    "              [1, 1]]) # 1 AND 1 = 1\n",
    "\n",
    "# Відповідні мітки класів (очікуваний результат для AND)\n",
    "y = np.array([[0], [0], [0], [1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Ініціалізація ваг та зміщення (bias)\n",
    "np.random.seed(1)\n",
    "weights = np.random.rand(2, 1)  # Два входи, один вихід\n",
    "bias = np.random.rand(1)  # Один зміщувальний коефіцієнт\n",
    "\n",
    "# Функція активації (сигмоида)\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Похідна сигмоїди для оновлення ваг\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Параметри навчання\n",
    "epochs = 5000  # Кількість ітерацій навчання\n",
    "learning_rate = 0.1  # Швидкість навчання\n",
    "errors = []  # Список для збереження середньої помилки\n",
    "\n",
    "# Навчання нейронної мережі\n",
    "for epoch in range(epochs):\n",
    "    # Прямий прохід\n",
    "    linear_output = np.dot(X, weights) + bias  # Зважена сума входів + зміщення\n",
    "    predicted_output = sigmoid(linear_output)  # Прогноз після функції активації\n",
    "\n",
    "    # Обчислення помилки\n",
    "    error = y - predicted_output  # Різниця між очікуваним і реальним виходом\n",
    "    errors.append(np.mean(np.abs(error)))  # Збереження середньої абсолютної помилки\n",
    "\n",
    "    # Зворотний прохід (градієнтний спуск)\n",
    "    d_predicted_output = error * sigmoid_derivative(predicted_output)  # Похідна помилки\n",
    "    weights += X.T.dot(d_predicted_output) * learning_rate  # Оновлення ваг\n",
    "    bias += np.sum(d_predicted_output) * learning_rate  # Оновлення зміщення\n",
    "\n",
    "    # Вивід прогресу кожні 1000 ітерацій\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Епоха {epoch}, Середня помилка: {np.mean(np.abs(error))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Прогнозування після навчання\n",
    "print(\"\\nПрогнозування після навчання:\")\n",
    "print(sigmoid(np.dot(X, weights) + bias))  # Вивід передбачених значень\n",
    "\n",
    "# Візуалізація помилки\n",
    "plt.plot(errors)\n",
    "plt.title(\"Зменшення помилки протягом навчання\")\n",
    "plt.xlabel(\"Епохи\")\n",
    "plt.ylabel(\"Середня абсолютна помилка\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
