{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SiracencoSerghei/DataScienceHW/blob/main/example_kaggle/les_12/Module_12_1_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e99d2d2-015d-4305-a487-7a0d0b36a263",
      "metadata": {
        "id": "6e99d2d2-015d-4305-a487-7a0d0b36a263"
      },
      "source": [
        "# Introduction to NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4967310-0a20-4ac9-a4ea-72b84ab84237",
      "metadata": {
        "id": "a4967310-0a20-4ac9-a4ea-72b84ab84237"
      },
      "source": [
        "Main building blocks of an NLP application:\n",
        "- tokenization\n",
        "- word embeddings\n",
        "- sequence modeling\n",
        "- common applications"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d401f27-7701-44f8-b6bb-0ff5771ee766",
      "metadata": {
        "id": "1d401f27-7701-44f8-b6bb-0ff5771ee766"
      },
      "source": [
        "### Lemmatization\n",
        " Lemmatization reduces words to their base or dictionary form (i.e., lemma)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a6d6f60-7b99-46bd-a614-cc3b97261bf1",
      "metadata": {
        "id": "5a6d6f60-7b99-46bd-a614-cc3b97261bf1",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')  # https://www.nltk.org/api/nltk.corpus.reader.wordnet.html\n",
        "\n",
        "# Sample text\n",
        "text = \"Running wild cats run in the forest. They ran away when they saw a pack of wolves.\"\n",
        "\n",
        "# Tokenize the text into words\n",
        "words = nltk.word_tokenize(text)\n",
        "# words = nltk.sent_tokenize(text)\n",
        "\n",
        "# Initialize WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Lemmatize each word\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "print(\"Original words:\", words)\n",
        "print(\"Lemmatized words:\", lemmatized_words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5057eaa-47d4-4774-8a84-0728c85e9416",
      "metadata": {
        "id": "a5057eaa-47d4-4774-8a84-0728c85e9416"
      },
      "source": [
        "### Stemming\n",
        "Stemming is a text processing technique that reduces words to their base or root form, called a stem.\n",
        "Stemming removes suffixes and prefixes from words to achieve this normalization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "363cc498-f594-4b5c-a687-ec6ff22da1bb",
      "metadata": {
        "id": "363cc498-f594-4b5c-a687-ec6ff22da1bb"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "# Sample text\n",
        "text = \"Running wild cats run in the forest. They ran away when they saw a pack of wolves.\"\n",
        "\n",
        "# Tokenize the text into words\n",
        "words = nltk.word_tokenize(text)\n",
        "\n",
        "# Initialize PorterStemmer\n",
        "stemmer = SnowballStemmer('english')\n",
        "\n",
        "# Stem each word\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "print(\"Original words:\", words)\n",
        "print(\"Stemmed words:\", stemmed_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "234187dc-9920-4507-8336-7cba7aec6a6f",
      "metadata": {
        "id": "234187dc-9920-4507-8336-7cba7aec6a6f"
      },
      "source": [
        "## Tokenization\n",
        "\n",
        "Splitting of texts into single unique units (tokens)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bae6ab29-41b3-49ea-9bfb-c0658efc68e7",
      "metadata": {
        "id": "bae6ab29-41b3-49ea-9bfb-c0658efc68e7"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from nltk.util import ngrams\n",
        "nltk.download('punkt')  # https://www.nltk.org/_modules/nltk/tokenize/punkt.html\n",
        "\n",
        "# Sample text\n",
        "text = \"Tokenization is an important step in natural language processing.\"\n",
        "\n",
        "# Character-level tokenization\n",
        "char_tokens = list(text)\n",
        "print(\"Character-level tokenization:\", char_tokens)\n",
        "\n",
        "# Word level tokenization\n",
        "word_tokens = word_tokenize(text)\n",
        "print(\"Word level tokenization:\", word_tokens)\n",
        "\n",
        "# N-gram level tokenization\n",
        "n = 2\n",
        "n_grams = list(ngrams(word_tokenize(text), n))\n",
        "print(\"N-gram level tokenization (2-grams):\", n_grams)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b45cd957-67ad-4485-b5af-ec9f44057694",
      "metadata": {
        "id": "b45cd957-67ad-4485-b5af-ec9f44057694"
      },
      "source": [
        "ByteLevelBPE as a Trained Tokenizer:\n",
        "https://towardsdatascience.com/byte-pair-encoding-subword-based-tokenization-algorithm-77828a70bee0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47f62e32-f98c-4adf-8ab8-4ffb48d9a21a",
      "metadata": {
        "id": "47f62e32-f98c-4adf-8ab8-4ffb48d9a21a"
      },
      "outputs": [],
      "source": [
        "# Trained tokenizer\n",
        "# BPE ensures that the most common words are represented in the vocabulary as a single token while the rare words are broken down\n",
        "# into two or more subword tokens and this is in agreement with what a subword-based tokenization algorithm does.\n",
        "\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "\n",
        "# Sample text\n",
        "text = \"\"\"Tokenization is an important step in natural language processing.\n",
        "Tokenization is an important step in NLP.\n",
        "Tokenization is an important process.\n",
        "Process of tokenizing text sequences is important.\"\"\"\n",
        "\n",
        "# Initialize ByteLevelBPETokenizer, used in GPT-2\n",
        "# https://towardsdatascience.com/byte-pair-encoding-subword-based-tokenization-algorithm-77828a70bee0\n",
        "tokenizer = ByteLevelBPETokenizer()\n",
        "\n",
        "# Train tokenizer on text\n",
        "tokenizer.train_from_iterator([text])\n",
        "\n",
        "# Sub-word level tokenization (Byte-pair encoding)\n",
        "subword_tokens = tokenizer.encode(text).tokens\n",
        "print(\"Sub-word level tokenization (Byte-pair encoding):\", subword_tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb694052-8b7c-4b15-b4e1-11b42e4c9dde",
      "metadata": {
        "id": "fb694052-8b7c-4b15-b4e1-11b42e4c9dde"
      },
      "outputs": [],
      "source": [
        "len(tokenizer.get_vocab())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f295eee-0894-467e-8b52-cfe410905795",
      "metadata": {
        "id": "2f295eee-0894-467e-8b52-cfe410905795",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "tokenizer.get_vocab()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a3209d0-7ebb-47b4-b927-e68f9de6a9dd",
      "metadata": {
        "id": "8a3209d0-7ebb-47b4-b927-e68f9de6a9dd"
      },
      "outputs": [],
      "source": [
        "tokenizer.id_to_token(260)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e39e25dd-7f0e-4356-94b3-8854e8cfddf0",
      "metadata": {
        "id": "e39e25dd-7f0e-4356-94b3-8854e8cfddf0"
      },
      "source": [
        "Training your own tokenizer: https://huggingface.co/learn/nlp-course/en/chapter6/8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5aaed23-2116-4bd3-94a0-98a006f18a57",
      "metadata": {
        "id": "c5aaed23-2116-4bd3-94a0-98a006f18a57"
      },
      "outputs": [],
      "source": [
        "# Training from scratch\n",
        "## WordPiece Tokenizer: https://huggingface.co/learn/nlp-course/en/chapter6/6\n",
        "\n",
        "from tokenizers import (\n",
        "    decoders,\n",
        "    models,\n",
        "    normalizers,\n",
        "    pre_tokenizers,\n",
        "    processors,\n",
        "    trainers,\n",
        "    Tokenizer,\n",
        ")\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n",
        "\n",
        "tokenizer.normalizer = normalizers.Sequence(\n",
        "    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]\n",
        ")\n",
        "\n",
        "print(tokenizer.normalizer.normalize_str(\"Héllò hôw are ü?\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18dd3015-b254-4202-a9af-995b389a84df",
      "metadata": {
        "id": "18dd3015-b254-4202-a9af-995b389a84df"
      },
      "outputs": [],
      "source": [
        "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
        "tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test my pre-tokenizer.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c438f38d-dfcb-4282-8715-c4d27d31470b",
      "metadata": {
        "id": "c438f38d-dfcb-4282-8715-c4d27d31470b",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "corpus = [\n",
        "    \"The quick brown fox jumps over the lazy dog\",\n",
        "    \"The cat sits on the window sill\",\n",
        "    \"The dog barks loudly in the night\",\n",
        "    \"Birds of a feather flock together\",\n",
        "    \"Tokenization is an important step in natural language processing.\",\n",
        "    \"Tokenization is an important step in NLP.\",\n",
        "    \"Tokenization is an important process.\",\n",
        "    \"Process of tokenizing text sequences is important.\"\n",
        "]\n",
        "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]  # BERT\n",
        "trainer = trainers.WordPieceTrainer(vocab_size=1000, special_tokens=special_tokens)\n",
        "tokenizer.train_from_iterator(corpus, trainer=trainer)\n",
        "tokenizer.decoder = decoders.WordPiece(prefix=\"##\")\n",
        "tokenizer.save(\"./tokenizer-trained.json\")\n",
        "#tokenizer = Tokenizer.from_file(\"./tokenizer-trained.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef88a6bb-a9c4-4da6-95d8-aa44fb31c462",
      "metadata": {
        "id": "ef88a6bb-a9c4-4da6-95d8-aa44fb31c462"
      },
      "outputs": [],
      "source": [
        "encoding = tokenizer.encode(\"The quick brown fox\")\n",
        "print(encoding.tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2061b70-cbb0-4e97-9954-27006a515a6f",
      "metadata": {
        "id": "d2061b70-cbb0-4e97-9954-27006a515a6f"
      },
      "outputs": [],
      "source": [
        "encoding.ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "949c5559-483b-44d0-acac-71586d2fb61c",
      "metadata": {
        "id": "949c5559-483b-44d0-acac-71586d2fb61c"
      },
      "outputs": [],
      "source": [
        "tokenizer.decode(encoding.ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ce50c8a-f62d-44a9-8eb3-6d4cb9372937",
      "metadata": {
        "id": "5ce50c8a-f62d-44a9-8eb3-6d4cb9372937",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "tokenizer.get_vocab()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd63dae7-9084-4ad6-a161-be3551c351ea",
      "metadata": {
        "id": "fd63dae7-9084-4ad6-a161-be3551c351ea"
      },
      "outputs": [],
      "source": [
        "tokenizer.get_vocab_size()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc6b2302-3acd-4c9d-bc74-be65e15798b0",
      "metadata": {
        "id": "fc6b2302-3acd-4c9d-bc74-be65e15798b0"
      },
      "source": [
        "# Text Embeddings\n",
        "\n",
        "https://www.deepset.ai/blog/the-beginners-guide-to-text-embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34b9feb1-6368-4405-ae8f-952eb096806b",
      "metadata": {
        "id": "34b9feb1-6368-4405-ae8f-952eb096806b"
      },
      "source": [
        "## Sparse (розріджені) vectors: One hot encoding, Bag of words, Tf-idf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2fb9c95-5dfe-4f66-ab23-120fa9f7429f",
      "metadata": {
        "id": "d2fb9c95-5dfe-4f66-ab23-120fa9f7429f"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "import nltk\n",
        "\n",
        "# Sample text\n",
        "text = \"The quick brown fox jumps over the lazy dog\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(text.lower())\n",
        "print(\"tokens\", tokens)\n",
        "# Create vocabulary\n",
        "vocab = sorted(set(tokens))\n",
        "print(\"vocab\", vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa728d8a-1d86-44d2-b98f-02d03abf471d",
      "metadata": {
        "id": "fa728d8a-1d86-44d2-b98f-02d03abf471d"
      },
      "outputs": [],
      "source": [
        "# Create one-hot encoding\n",
        "one_hot_encoded = []\n",
        "for token in tokens:\n",
        "    one_hot_vector = [0] * len(vocab)\n",
        "    one_hot_vector[vocab.index(token)] = 1\n",
        "    one_hot_encoded.append(one_hot_vector)\n",
        "one_hot_encoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9efe46ba-3fc1-40e3-b4ae-45273a9d7068",
      "metadata": {
        "id": "9efe46ba-3fc1-40e3-b4ae-45273a9d7068"
      },
      "outputs": [],
      "source": [
        "# Convert to numpy array for easier manipulation\n",
        "one_hot_encoded = np.array(one_hot_encoded)\n",
        "\n",
        "# Print results\n",
        "print(\"Original text:\", text)\n",
        "print(\"Tokens:\", tokens)\n",
        "print(\"Vocabulary:\", vocab)\n",
        "print(\"One-hot encoded text:\\n\", one_hot_encoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b62ce7ac-987a-4448-abc6-ec6456918a3c",
      "metadata": {
        "id": "b62ce7ac-987a-4448-abc6-ec6456918a3c"
      },
      "outputs": [],
      "source": [
        "# Bag of words\n",
        "\n",
        "# Initialize Bag of Words\n",
        "bow = np.zeros(len(vocab))\n",
        "\n",
        "# Count occurrences of each word\n",
        "for token in tokens:\n",
        "    bow[vocab.index(token)] += 1\n",
        "\n",
        "# Print results\n",
        "print(\"Original text:\", text)\n",
        "print(\"Tokens:\", tokens)\n",
        "print(\"Vocabulary:\", vocab)\n",
        "print(\"Bag of Words:\\n\", bow)\n",
        "\n",
        "# And just like that, we have managed to encode our text as a vector\n",
        "# (also known as a “bag of words” or “BoW” embedding — because it ignores the order of the words in the sentence)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3310fa21-a40b-4687-9fc8-18acfc7d2c6f",
      "metadata": {
        "id": "3310fa21-a40b-4687-9fc8-18acfc7d2c6f"
      },
      "outputs": [],
      "source": [
        "# TF-idf\n",
        "# see module 7 lesson 1\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Sample corpus\n",
        "corpus = [\n",
        "    \"The quick brown fox jumps over the lazy dog\",\n",
        "    \"The cat sits on the window sill\",\n",
        "    \"The dog barks loudly in the night\",\n",
        "    \"Birds of a feather flock together\"\n",
        "]\n",
        "\n",
        "# Initialize TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the corpus\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Get feature names (vocabulary)\n",
        "vocab = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Convert TF-IDF matrix to dense array for easier manipulation\n",
        "tfidf_matrix_dense = tfidf_matrix.toarray()\n",
        "\n",
        "# Print results\n",
        "print(\"Sample corpus:\", corpus)\n",
        "print(\"Vocabulary:\", vocab)\n",
        "print(\"TF-IDF matrix:\\n\", tfidf_matrix_dense)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f2d2720-f916-43ac-b2c8-cae5360734b7",
      "metadata": {
        "id": "0f2d2720-f916-43ac-b2c8-cae5360734b7"
      },
      "source": [
        "### Cosine similarity\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e36a7d2c-bc86-4848-83e5-9d4c874a7df9",
      "metadata": {
        "id": "e36a7d2c-bc86-4848-83e5-9d4c874a7df9"
      },
      "source": [
        "![cosine](https://storage.googleapis.com/lds-media/images/cosine-similarity-vectors.original.jpg)\n",
        "Source: https://www.learndatasci.com/glossary/cosine-similarity/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1a9e075-7500-4714-a3fd-396ecb8ee86b",
      "metadata": {
        "id": "c1a9e075-7500-4714-a3fd-396ecb8ee86b"
      },
      "outputs": [],
      "source": [
        "# show vector similarity\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Sample vectors\n",
        "vector1 = np.array([1, 2, 3])\n",
        "vector2 = np.array([4, 5, 6])\n",
        "\n",
        "# Reshape the vectors to ensure they are 2D arrays\n",
        "vector1 = vector1.reshape(1, -1)\n",
        "vector2 = vector2.reshape(1, -1)\n",
        "\n",
        "# Compute cosine similarity\n",
        "similarity = cosine_similarity(vector1, vector2)\n",
        "\n",
        "print(\"Vector 1:\", vector1)\n",
        "print(\"Vector 2:\", vector2)\n",
        "print(\"Cosine similarity:\", similarity[0][0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43648316-28c7-4fec-a1de-e303ec18c42a",
      "metadata": {
        "id": "43648316-28c7-4fec-a1de-e303ec18c42a"
      },
      "source": [
        "![formula](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LfW66-WsYkFqWc4XYJbEJg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/greyhatguy007/deep-learning-specialization/blob/main/C5-sequence-models/week2/C5W2A1/Operations_on_word_vectors_v2a.ipynb"
      ],
      "metadata": {
        "id": "xdBmrzEAfLkq"
      },
      "id": "xdBmrzEAfLkq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c49fe42-287e-42a6-975b-2516b1cafdc2",
      "metadata": {
        "id": "9c49fe42-287e-42a6-975b-2516b1cafdc2"
      },
      "outputs": [],
      "source": [
        "def cosine_similarity(u, v):\n",
        "    \"\"\"\n",
        "    Cosine similarity reflects the degree of similarity between u and v\n",
        "\n",
        "    Arguments:\n",
        "        u -- a word vector of shape (n,)\n",
        "        v -- a word vector of shape (n,)\n",
        "\n",
        "    Returns:\n",
        "        cosine_similarity -- the cosine similarity between u and v defined by the formula above.\n",
        "    \"\"\"\n",
        "\n",
        "    # Special case. Consider the case u = [0, 0], v=[0, 0]\n",
        "    if np.all(u == v):\n",
        "        return 1\n",
        "\n",
        "    # Compute the dot product between u and v (≈1 line)\n",
        "    dot = np.dot(u, v)\n",
        "    # Compute the L2 norm of u (≈1 line)\n",
        "    norm_u = np.sqrt(np.sum(np.dot(u,u)))\n",
        "\n",
        "    # Compute the L2 norm of v (≈1 line)\n",
        "    norm_v = np.sqrt(np.sum(np.dot(v,v)))\n",
        "\n",
        "    # Avoid division by 0\n",
        "    if np.isclose(norm_u * norm_v, 0, atol=1e-32):\n",
        "        return 0\n",
        "\n",
        "    # Compute the cosine similarity defined by formula (1) (≈1 line)\n",
        "    cosine_similarity = dot / (norm_u * norm_v)\n",
        "\n",
        "    return cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d96a5729-c023-4cbb-9359-4c7e0063ff3e",
      "metadata": {
        "id": "d96a5729-c023-4cbb-9359-4c7e0063ff3e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Sample text vectors\n",
        "vector1 = np.array([1, 2])\n",
        "vector2 = np.array([4, 5])\n",
        "\n",
        "# Reshape the vectors to ensure they are 2D arrays\n",
        "vector1 = vector1.reshape(1, -1)\n",
        "vector2 = vector2.reshape(1, -1)\n",
        "\n",
        "# Compute cosine similarity\n",
        "similarity = cosine_similarity(vector1, vector2)[0][0]\n",
        "\n",
        "# Plot vectors\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot([0, vector1[0][0]], [0, vector1[0][1]], label='Vector 1', marker='o')\n",
        "plt.plot([0, vector2[0][0]], [0, vector2[0][1]], label='Vector 2', marker='o')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "plt.title('Visualization of Text Vectors')\n",
        "plt.legend()\n",
        "\n",
        "# Annotate cosine similarity\n",
        "plt.text(1, 4, f'Cosine Similarity: {similarity:.2f}', fontsize=12)\n",
        "\n",
        "# Set aspect ratio to be equal\n",
        "plt.gca().set_aspect('equal', adjustable='box')\n",
        "\n",
        "# Show plot\n",
        "plt.grid()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb49c273-79fa-437f-8a44-ab94535e5173",
      "metadata": {
        "id": "bb49c273-79fa-437f-8a44-ab94535e5173"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Sample corpus\n",
        "corpus = [\n",
        "    \"The quick brown fox jumps over the lazy dog\",\n",
        "    \"The cat sits on the window sill\",\n",
        "    \"The dog barks loudly in the night\",\n",
        "    \"Birds of a feather flock together\"\n",
        "]\n",
        "\n",
        "# Initialize TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the corpus\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Convert TF-IDF matrix to dense array for easier manipulation\n",
        "tfidf_matrix_dense = tfidf_matrix.toarray()\n",
        "\n",
        "# Compute cosine similarity\n",
        "cos_sim = cosine_similarity(tfidf_matrix_dense)\n",
        "\n",
        "# Perform PCA to reduce dimensions to 2 for visualization\n",
        "pca = PCA(n_components=2)\n",
        "tfidf_matrix_pca = pca.fit_transform(tfidf_matrix_dense)\n",
        "\n",
        "# Plot one graph per pair of text\n",
        "num_pairs = len(corpus) * (len(corpus) - 1) // 2\n",
        "plt.figure(figsize=(15, 10))\n",
        "subplot_index = 1\n",
        "for i in range(len(corpus)):\n",
        "    for j in range(i + 1, len(corpus)):\n",
        "        plt.subplot(2, num_pairs // 2, subplot_index)\n",
        "        plt.scatter(tfidf_matrix_pca[:, 0], tfidf_matrix_pca[:, 1])\n",
        "        plt.plot([tfidf_matrix_pca[i, 0], tfidf_matrix_pca[j, 0]],\n",
        "                 [tfidf_matrix_pca[i, 1], tfidf_matrix_pca[j, 1]],\n",
        "                 linestyle='-', color='red')\n",
        "        plt.xlabel('Principal Component 1')\n",
        "        plt.ylabel('Principal Component 2')\n",
        "        plt.title(f'Cosine Similarity: {cos_sim[i, j]:.2f}')\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        subplot_index += 1\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b6c6036-825a-4277-962a-31f3d8a979ca",
      "metadata": {
        "id": "3b6c6036-825a-4277-962a-31f3d8a979ca"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Sample corpus\n",
        "corpus = [\n",
        "    \"The quick brown fox jumps over the lazy dog\",\n",
        "    \"The cat sits on the window sill\",\n",
        "    \"The dog barks loudly in the night\",\n",
        "    \"Birds of a feather flock together\"\n",
        "]\n",
        "\n",
        "# Initialize TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the corpus\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Convert TF-IDF matrix to dense array for easier manipulation\n",
        "tfidf_matrix_dense = tfidf_matrix.toarray()\n",
        "\n",
        "# Compute cosine similarity\n",
        "cos_sim = cosine_similarity(tfidf_matrix_dense)\n",
        "\n",
        "for i in range(len(corpus)):\n",
        "    for j in range(i + 1, len(corpus)):\n",
        "        print(f'Text: {corpus[i]}, \\n{corpus[j]}\\nCosine Similarity: {cos_sim[i, j]:.2f}.\\n')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08ee51e7-956e-47d8-a738-e1753cd0a43b",
      "metadata": {
        "id": "08ee51e7-956e-47d8-a738-e1753cd0a43b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Sample corpus\n",
        "corpus = [\n",
        "    \"The quick brown fox jumps over the lazy dog\",\n",
        "    \"The cat sits on the window sill\",\n",
        "    \"The dog barks loudly in the night\",\n",
        "    \"Birds of a feather flock together\"\n",
        "]\n",
        "\n",
        "# Initialize TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the corpus\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Convert TF-IDF matrix to dense array for easier manipulation\n",
        "tfidf_matrix_dense = tfidf_matrix.toarray()\n",
        "\n",
        "# Perform PCA to reduce dimensions to 2 for visualization\n",
        "pca = PCA(n_components=2)\n",
        "tfidf_matrix_pca = pca.fit_transform(tfidf_matrix_dense)\n",
        "\n",
        "# Compute cosine similarity\n",
        "cos_sim = cosine_similarity(tfidf_matrix_pca)\n",
        "\n",
        "# Plot one graph per pair of text\n",
        "num_pairs = len(corpus) * (len(corpus) - 1) // 2\n",
        "plt.figure(figsize=(15, 10))\n",
        "subplot_index = 1\n",
        "for i in range(len(corpus)):\n",
        "    for j in range(i + 1, len(corpus)):\n",
        "        plt.subplot(2, num_pairs // 2, subplot_index)\n",
        "        plt.scatter(tfidf_matrix_pca[:, 0], tfidf_matrix_pca[:, 1])\n",
        "        plt.plot([0, tfidf_matrix_pca[i, 0]],\n",
        "                 [0, tfidf_matrix_pca[i, 1]],\n",
        "                 linestyle='-', color='blue', alpha=0.5)\n",
        "        plt.plot([0, tfidf_matrix_pca[j, 0]],\n",
        "                 [0, tfidf_matrix_pca[j, 1]],\n",
        "                 linestyle='-', color='red', alpha=0.5)\n",
        "        plt.xlabel('Principal Component 1')\n",
        "        plt.ylabel('Principal Component 2')\n",
        "        plt.title(f'Cosine Similarity: {cos_sim[i, j]:.2f}.\\nText: {corpus[i]}, \\n{corpus[j]}')\n",
        "        plt.xticks(np.arange(-0.5, 1.5, 0.5))\n",
        "        plt.yticks(np.arange(-0.5, 1.5, 0.5))\n",
        "        subplot_index += 1\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "872607d3-09af-4722-b31e-2f34ddc808e4",
      "metadata": {
        "id": "872607d3-09af-4722-b31e-2f34ddc808e4"
      },
      "source": [
        "\n",
        "## Word2Vec\n",
        "https://www.tensorflow.org/text/tutorials/word2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db4d9f05-7a47-483f-abda-bfef1a850fe0",
      "metadata": {
        "id": "db4d9f05-7a47-483f-abda-bfef1a850fe0"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "# Sample corpus\n",
        "corpus = [\n",
        "    \"The quick brown fox jumps over the lazy dog\",\n",
        "    \"The cat sits on the window sill\",\n",
        "    \"The dog barks loudly in the night\",\n",
        "    \"Birds of a feather flock together\"\n",
        "]\n",
        "\n",
        "# Tokenize the corpus\n",
        "tokenized_corpus = [word_tokenize(doc.lower()) for doc in corpus]\n",
        "\n",
        "# Define Word2Vec model parameters\n",
        "vector_size = 100  # Dimensionality of word vectors\n",
        "window_size = 5  # Context window size\n",
        "min_count = 1  # Minimum frequency count of words\n",
        "\n",
        "# Initialize Word2Vec model\n",
        "model = Word2Vec(\n",
        "    sentences=tokenized_corpus,\n",
        "    vector_size=vector_size,\n",
        "    window=window_size,\n",
        "    min_count=min_count,\n",
        "    sg=1  # Use Skip-gram model\n",
        ")\n",
        "\n",
        "# Train Word2Vec model\n",
        "model.train(tokenized_corpus, total_examples=len(tokenized_corpus), epochs=10)\n",
        "\n",
        "# Save trained model\n",
        "model.save(\"word2vec_model.bin\")\n",
        "\n",
        "# To load the trained model:\n",
        "# loaded_model = Word2Vec.load(\"word2vec_model.bin\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e6390a2-fc4b-47ed-a2ea-6443e7753c3b",
      "metadata": {
        "id": "6e6390a2-fc4b-47ed-a2ea-6443e7753c3b"
      },
      "outputs": [],
      "source": [
        "model.get_latest_training_loss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62e49a33-41b4-4b84-bcf6-ce348c59ecb6",
      "metadata": {
        "id": "62e49a33-41b4-4b84-bcf6-ce348c59ecb6"
      },
      "outputs": [],
      "source": [
        "vocab = model.wv.index_to_key\n",
        "vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55158a2f-4c74-4ac7-8412-a65a97df6424",
      "metadata": {
        "id": "55158a2f-4c74-4ac7-8412-a65a97df6424"
      },
      "outputs": [],
      "source": [
        "model.wv.key_to_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0b7878d-b1f0-40a9-b635-da38851f23be",
      "metadata": {
        "id": "d0b7878d-b1f0-40a9-b635-da38851f23be"
      },
      "outputs": [],
      "source": [
        "# Sample text to encode\n",
        "text = \"This fox is a very quick fox\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(text.lower())\n",
        "\n",
        "# Encode the text\n",
        "word_vectors = [model.wv[word] for word in tokens if word in model.wv.key_to_index.keys()]\n",
        "\n",
        "# Average the word vectors to get the text encoding\n",
        "if word_vectors:\n",
        "    encoded_text = np.mean(word_vectors, axis=0)\n",
        "else:\n",
        "    encoded_text = np.zeros(model.vector_size)  # Default encoding for empty text\n",
        "\n",
        "# Print the encoded text\n",
        "print(\"Encoded text:\", encoded_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9788fb60-7dcc-4559-8de2-b34908c90147",
      "metadata": {
        "id": "9788fb60-7dcc-4559-8de2-b34908c90147"
      },
      "outputs": [],
      "source": [
        "model.wv[\"fox\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c25601a-0e03-4e0a-8b25-933005d7bd13",
      "metadata": {
        "id": "8c25601a-0e03-4e0a-8b25-933005d7bd13"
      },
      "outputs": [],
      "source": [
        "word_vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22ec35aa-f805-4964-b7f6-5004e9bf33b5",
      "metadata": {
        "id": "22ec35aa-f805-4964-b7f6-5004e9bf33b5"
      },
      "outputs": [],
      "source": [
        "# Sample corpus\n",
        "corpus = [\n",
        "    \"The quick brown fox jumps over the lazy dog\",\n",
        "    \"The cat sits on the window sill\",\n",
        "    \"The dog barks loudly in the night\",\n",
        "    \"Birds of a feather flock together\"\n",
        "]\n",
        "\n",
        "embeddings = np.zeros((len(corpus), model.vector_size))\n",
        "\n",
        "for i in range(len(corpus)):\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(corpus[i].lower())\n",
        "\n",
        "    # Encode the text\n",
        "    word_vectors = [model.wv[word] for word in tokens if word in model.wv.key_to_index.keys()]\n",
        "\n",
        "    # Average the word vectors to get the text encoding\n",
        "    if word_vectors:\n",
        "        encoded_text = np.mean(word_vectors, axis=0)\n",
        "    else:\n",
        "        encoded_text = np.zeros(model.vector_size)  # Default encoding for empty text\n",
        "    embeddings[i] = encoded_text\n",
        "\n",
        "print(embeddings)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5957f1e6-4438-41be-91ed-4ce9a49101d0",
      "metadata": {
        "id": "5957f1e6-4438-41be-91ed-4ce9a49101d0"
      },
      "outputs": [],
      "source": [
        "# Compute cosine similarity\n",
        "cos_sim = cosine_similarity(embeddings)\n",
        "for i in range(len(corpus)):\n",
        "    for j in range(i + 1, len(corpus)):\n",
        "        print(f'Text: {corpus[i]}, \\n{corpus[j]}\\nCosine Similarity: {cos_sim[i, j]:.2f}.\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "663f2a9c-15e2-429a-99ab-7ca0966b8c82",
      "metadata": {
        "id": "663f2a9c-15e2-429a-99ab-7ca0966b8c82"
      },
      "outputs": [],
      "source": [
        "cos_sim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4d44063-cd06-4a6a-a586-adf37d907b81",
      "metadata": {
        "id": "f4d44063-cd06-4a6a-a586-adf37d907b81"
      },
      "outputs": [],
      "source": [
        "# Training of Skip-gram model\n",
        "# source: https://colab.research.google.com/drive/1IxAnnFSqk3mL3A8n1PKYWdEzDSd2Y9rF?usp=sharing#scrollTo=13xBa01XEnpb\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, Flatten\n",
        "\n",
        "# Set up the training data\n",
        "sentences = [[\"I\", \"love\", \"machine\", \"learning\"],\n",
        "             [\"I\", \"like\", \"deep\", \"learning\"],\n",
        "             [\"I\", \"enjoy\", \"neural\", \"networks\"]]\n",
        "\n",
        "# Create the skip-gram dataset\n",
        "skip_gram_pairs = []\n",
        "window_size = 2\n",
        "\n",
        "for sentence in sentences:\n",
        "    for i in range(len(sentence)):\n",
        "        target_word = sentence[i]\n",
        "        for j in range(i - window_size, i + window_size + 1):\n",
        "            if j >= 0 and j < len(sentence) and j != i:\n",
        "                context_word = sentence[j]\n",
        "                skip_gram_pairs.append((target_word, context_word))\n",
        "\n",
        "# Create word-to-index and index-to-word mappings\n",
        "word_to_index = {}\n",
        "index_to_word = {}\n",
        "index = 0\n",
        "\n",
        "for sentence in sentences:\n",
        "    for word in sentence:\n",
        "        if word not in word_to_index:\n",
        "            word_to_index[word] = index\n",
        "            index_to_word[index] = word\n",
        "            index += 1\n",
        "\n",
        "# Convert skip-gram pairs to indices\n",
        "skip_gram_pairs_indices = []\n",
        "for pair in skip_gram_pairs:\n",
        "    target_index = word_to_index[pair[0]]\n",
        "    context_index = word_to_index[pair[1]]\n",
        "    skip_gram_pairs_indices.append((target_index, context_index))\n",
        "\n",
        "# Define the model\n",
        "vocab_size = len(word_to_index)\n",
        "embedding_dim = 10\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim, input_length=1))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "# Compile and train the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "target_words = []\n",
        "context_words = []\n",
        "\n",
        "for pair in skip_gram_pairs_indices:\n",
        "    target_words.append(pair[0])\n",
        "    context_words.append(pair[1])\n",
        "\n",
        "target_words = np.array(target_words)\n",
        "context_words = np.array(context_words)\n",
        "\n",
        "model.fit(target_words, tf.keras.utils.to_categorical(context_words, num_classes=vocab_size), epochs=100)\n",
        "\n",
        "# Get the word embeddings\n",
        "embeddings = model.get_weights()[0]\n",
        "\n",
        "# Print the word embeddings\n",
        "for i in range(vocab_size):\n",
        "    word = index_to_word[i]\n",
        "    embedding = embeddings[i]\n",
        "    print(f\"Word: {word}, Embedding: {embedding}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccd764d9-3293-4465-a9bc-4277b0b412eb",
      "metadata": {
        "id": "ccd764d9-3293-4465-a9bc-4277b0b412eb"
      },
      "source": [
        "### Pretrained embeddings: GloVe\n",
        "\n",
        "https://nlp.stanford.edu/projects/glove/\n",
        "\n",
        "https://medium.com/analytics-vidhya/basics-of-using-pre-trained-glove-vectors-in-python-d38905f356db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01e0e0c4-af44-4d2b-a666-1eb567f137c0",
      "metadata": {
        "id": "01e0e0c4-af44-4d2b-a666-1eb567f137c0"
      },
      "outputs": [],
      "source": [
        "embeddings_dict = {}\n",
        "with open(\"glove.6B.50d.txt\", 'r', encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vector = np.asarray(values[1:], \"float32\")\n",
        "        embeddings_dict[word] = vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd841a5f-b524-4450-a8db-49dbe79c0edf",
      "metadata": {
        "id": "cd841a5f-b524-4450-a8db-49dbe79c0edf"
      },
      "outputs": [],
      "source": [
        "len(embeddings_dict[\"the\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fb95a00-fcbc-40d3-8777-3ad8fd2858e0",
      "metadata": {
        "id": "4fb95a00-fcbc-40d3-8777-3ad8fd2858e0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy import spatial\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "def find_closest_embeddings(embedding):\n",
        "    return sorted(embeddings_dict.keys(), key=lambda word: spatial.distance.euclidean(embeddings_dict[word], embedding))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27818c52-06a9-4eae-b205-abfdc64242a1",
      "metadata": {
        "id": "27818c52-06a9-4eae-b205-abfdc64242a1"
      },
      "outputs": [],
      "source": [
        "find_closest_embeddings(embeddings_dict[\"king\"])[1:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c4b78bd-400e-4a7d-9245-3229e6adea7a",
      "metadata": {
        "id": "7c4b78bd-400e-4a7d-9245-3229e6adea7a"
      },
      "outputs": [],
      "source": [
        "find_closest_embeddings(\n",
        "        embeddings_dict[\"king\"] - embeddings_dict[\"man\"] + embeddings_dict[\"woman\"]\n",
        ")[1:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "018832a5-b9b4-4e1a-ae82-603791a443b7",
      "metadata": {
        "id": "018832a5-b9b4-4e1a-ae82-603791a443b7"
      },
      "outputs": [],
      "source": [
        "find_closest_embeddings(\n",
        "        embeddings_dict[\"ukraine\"] - embeddings_dict[\"country\"] + embeddings_dict[\"city\"]\n",
        ")[1:7]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33b6ccfe-5a57-4766-829e-05841aeeb910",
      "metadata": {
        "id": "33b6ccfe-5a57-4766-829e-05841aeeb910"
      },
      "outputs": [],
      "source": [
        "find_closest_embeddings(\n",
        "        embeddings_dict[\"germany\"] - embeddings_dict[\"country\"] + embeddings_dict[\"city\"]\n",
        ")[1:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a04c21bb-3d75-43d9-8f81-cfc274917b9a",
      "metadata": {
        "id": "a04c21bb-3d75-43d9-8f81-cfc274917b9a"
      },
      "outputs": [],
      "source": [
        "find_closest_embeddings(\n",
        "        embeddings_dict[\"cat\"] - embeddings_dict[\"home\"] + embeddings_dict[\"wild\"]\n",
        ")[1:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37bcfd54-17f1-4656-ad4f-b870708f0474",
      "metadata": {
        "id": "37bcfd54-17f1-4656-ad4f-b870708f0474"
      },
      "outputs": [],
      "source": [
        "find_closest_embeddings(\n",
        "        embeddings_dict[\"kiev\"] - embeddings_dict[\"ukraine\"] + embeddings_dict[\"germany\"]\n",
        ")[1:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d8c60ee-fcd0-4878-afa1-c8c359ac12d5",
      "metadata": {
        "id": "0d8c60ee-fcd0-4878-afa1-c8c359ac12d5"
      },
      "outputs": [],
      "source": [
        "find_closest_embeddings(\n",
        "        embeddings_dict[\"bird\"] - embeddings_dict[\"air\"] + embeddings_dict[\"water\"]\n",
        ")[1:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a2ba7f9-3571-43b7-9c11-7375ff5d5d30",
      "metadata": {
        "id": "1a2ba7f9-3571-43b7-9c11-7375ff5d5d30"
      },
      "outputs": [],
      "source": [
        "find_closest_embeddings(\n",
        "        embeddings_dict[\"windows\"] - embeddings_dict[\"microsoft\"] + embeddings_dict[\"apple\"]\n",
        ")[1:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "569d1268-3d6a-4f23-98c4-48e16936b9b9",
      "metadata": {
        "id": "569d1268-3d6a-4f23-98c4-48e16936b9b9"
      },
      "outputs": [],
      "source": [
        "# Sample corpus\n",
        "corpus = [\n",
        "    \"The quick brown fox jumps over the lazy dog\",\n",
        "    \"The cat sits on the window sill\",\n",
        "    \"The dog barks loudly in the night\",\n",
        "    \"Birds of a feather flock together\"\n",
        "]\n",
        "\n",
        "embeddings = np.zeros((len(corpus), 50))\n",
        "\n",
        "for i in range(len(corpus)):\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(corpus[i].lower())\n",
        "\n",
        "    # Encode the text\n",
        "    word_vectors = [embeddings_dict[word] for word in tokens if word in embeddings_dict]\n",
        "\n",
        "    # Average the word vectors to get the text encoding\n",
        "    if word_vectors:\n",
        "        encoded_text = np.mean(word_vectors, axis=0)\n",
        "    else:\n",
        "        encoded_text = np.zeros(model.vector_size)  # Default encoding for empty text\n",
        "    embeddings[i] = encoded_text\n",
        "\n",
        "print(embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a831bef8-22e8-457f-a890-b7bce412a1a7",
      "metadata": {
        "id": "a831bef8-22e8-457f-a890-b7bce412a1a7"
      },
      "outputs": [],
      "source": [
        "cos_sim = cosine_similarity(embeddings)\n",
        "for i in range(len(corpus)):\n",
        "    for j in range(i + 1, len(corpus)):\n",
        "        print(f'Text: {corpus[i]}, \\n{corpus[j]}\\nCosine Similarity: {cos_sim[i, j]:.2f}.\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a85e0dbb-bcca-41be-86b8-3b8b5a6e6dcb",
      "metadata": {
        "id": "a85e0dbb-bcca-41be-86b8-3b8b5a6e6dcb"
      },
      "outputs": [],
      "source": [
        "# Perform PCA to reduce dimensions to 2 for visualization\n",
        "pca = PCA(n_components=2)\n",
        "matrix_pca = pca.fit_transform(embeddings)\n",
        "# Compute cosine similarity\n",
        "cos_sim = cosine_similarity(matrix_pca)\n",
        "\n",
        "\n",
        "# Plot one graph per pair of text\n",
        "num_pairs = len(corpus) * (len(corpus) - 1) // 2\n",
        "plt.figure(figsize=(15, 10))\n",
        "subplot_index = 1\n",
        "for i in range(len(corpus)):\n",
        "    for j in range(i + 1, len(corpus)):\n",
        "        plt.subplot(2, num_pairs // 2, subplot_index)\n",
        "        plt.scatter(matrix_pca[:, 0], matrix_pca[:, 1])\n",
        "        plt.plot([0, matrix_pca[i, 0]],\n",
        "                 [0, matrix_pca[i, 1]],\n",
        "                 linestyle='-', color='blue', alpha=0.5)\n",
        "        plt.plot([0, matrix_pca[j, 0]],\n",
        "                 [0, matrix_pca[j, 1]],\n",
        "                 linestyle='-', color='red', alpha=0.5)\n",
        "        plt.xlabel('Principal Component 1')\n",
        "        plt.ylabel('Principal Component 2')\n",
        "        plt.title(f'Cosine Similarity: {cos_sim[i, j]:.2f}.\\nText: {corpus[i]}, \\n{corpus[j]}')\n",
        "        plt.xticks(np.arange(-0.5, 1.5, 0.5))\n",
        "        plt.yticks(np.arange(-0.5, 1.5, 0.5))\n",
        "        subplot_index += 1\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc8467a5-ef7d-4c07-9cf9-c2dddaef851a",
      "metadata": {
        "id": "dc8467a5-ef7d-4c07-9cf9-c2dddaef851a"
      },
      "outputs": [],
      "source": [
        "# Training Glove Embeddings\n",
        "# Source: https://colab.research.google.com/drive/1IxAnnFSqk3mL3A8n1PKYWdEzDSd2Y9rF?usp=sharing#scrollTo=DpONsaktz-8w\n",
        "# https://medium.com/nerd-for-tech/implementing-glove-from-scratch-word-embedding-for-transformers-95503138d65\n",
        "#\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "corpus = [\n",
        "    \"I love chocolate\",\n",
        "    \"I love ice cream\",\n",
        "    \"I enjoy playing tennis\"\n",
        "]\n",
        "# Initialize vocabulary and co-occurrence matrix\n",
        "vocab = set()\n",
        "co_occurrence = defaultdict(float)\n",
        "\n",
        "window_size = 4\n",
        "# Iterate through the corpus to build vocabulary and co-occurrence matrix\n",
        "for sentence in corpus:\n",
        "    words = sentence.split()\n",
        "    for i in range(len(words)):\n",
        "        word = words[i]\n",
        "        vocab.add(word)\n",
        "        for j in range(max(0, i - window_size), min(i + window_size + 1, len(words))):\n",
        "            if i != j:\n",
        "                co_occurrence[(word, words[j])] += 1.0 / abs(i - j)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7df9ca0-06d9-4ad3-98d8-b883abeff83e",
      "metadata": {
        "id": "d7df9ca0-06d9-4ad3-98d8-b883abeff83e"
      },
      "outputs": [],
      "source": [
        "co_occurrence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23c9c9f9-5021-4413-82c0-6e062f6b4488",
      "metadata": {
        "id": "23c9c9f9-5021-4413-82c0-6e062f6b4488"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 10\n",
        "word_embeddings = {\n",
        "    word: np.random.randn(embedding_dim) for word in vocab\n",
        "}\n",
        "\n",
        "learning_rate = 0.1\n",
        "num_epochs = 100\n",
        "\n",
        "# Gradient descent to update word embeddings\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for (word_i, word_j), observed_count in co_occurrence.items():\n",
        "        # Calculate dot product of word embeddings\n",
        "        dot_product = np.dot(word_embeddings[word_i], word_embeddings[word_j])\n",
        "\n",
        "        # Calculate difference and update\n",
        "        diff = dot_product - np.log(observed_count)\n",
        "        total_loss += 0.5 * diff**2\n",
        "        gradient = diff * word_embeddings[word_j]\n",
        "        word_embeddings[word_i] -= learning_rate * gradient\n",
        "\n",
        "    print(f\"Epoch: {epoch+1}, Loss: {total_loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dff9c0b5-df84-4336-8fe0-a99fa678128e",
      "metadata": {
        "id": "dff9c0b5-df84-4336-8fe0-a99fa678128e"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}