{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SiracencoSerghei/DataScienceHW/blob/main/example_kaggle/les_12/Module_12_2_Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "492fa949-1e80-4b41-a57f-954ad52301d6",
      "metadata": {
        "id": "492fa949-1e80-4b41-a57f-954ad52301d6"
      },
      "source": [
        "# Питання з таблиці\n",
        "## Data Science project guides\n",
        "\n",
        "- Data Camp: https://www.datacamp.com/blog/data-science-use-cases-guide\n",
        "- 10 Steps: https://towardsdatascience.com/10-steps-to-pass-the-data-science-take-home-task-c5a696679526\n",
        "- Notebook Template: https://medium.com/@jacowp357/12-steps-towards-implementing-a-data-science-solution-d2ce196f22c1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed5fc8a1-18aa-4bfd-9b31-ed6c2f0dde56",
      "metadata": {
        "id": "ed5fc8a1-18aa-4bfd-9b31-ed6c2f0dde56"
      },
      "source": [
        "## Data Science Interview Questions\n",
        "- Data Camp: https://www.datacamp.com/blog/data-scientist-interview-questions\n",
        "- https://www.springboard.com/blog/data-science/data-science-interview-questions/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bde9d9c1-fd66-46fa-8211-33816c8ed707",
      "metadata": {
        "id": "bde9d9c1-fd66-46fa-8211-33816c8ed707"
      },
      "source": [
        "# Transformer models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6494006e-bff8-41cd-b928-1e7e195d29b6",
      "metadata": {
        "id": "6494006e-bff8-41cd-b928-1e7e195d29b6"
      },
      "source": [
        "## Before transformers: Static Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9fad40b-a927-41b7-9e9b-3486b26c41a2",
      "metadata": {
        "id": "a9fad40b-a927-41b7-9e9b-3486b26c41a2"
      },
      "source": [
        "### One hot encoding\n",
        "\n",
        "![one-hot](https://miro.medium.com/v2/resize:fit:1400/format:webp/0*T5jaa2othYfXZX9W.)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04d64f00-2ed2-4488-bd5b-c4848e32b2ea",
      "metadata": {
        "id": "04d64f00-2ed2-4488-bd5b-c4848e32b2ea"
      },
      "source": [
        "### Word2Vec\n",
        "\n",
        "![word2vec-vec](http://jalammar.github.io/images/word2vec/word2vec.png)\n",
        "\n",
        "![word2vec](https://miro.medium.com/v2/resize:fit:1394/format:webp/0*XMW5mf81LSHodnTi.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2eb119ed-01f4-4570-8387-b8c1cdd89eab",
      "metadata": {
        "id": "2eb119ed-01f4-4570-8387-b8c1cdd89eab"
      },
      "source": [
        "### Проблема 1: статичних Embeddings недостатньо\n",
        "\n",
        "- Приклад 1:\n",
        "\"Panda\" + \"Kung Fu\" == \"Kung Fu Panda\" ?\n",
        "\n",
        "Панда (в дикій природі), яка знає Кунг-фу != конкретний персонаж мультфільму \"Панда Кунг-Фу\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c2f2294-cfc4-4bf5-84e6-37665d2a0f97",
      "metadata": {
        "id": "6c2f2294-cfc4-4bf5-84e6-37665d2a0f97"
      },
      "source": [
        "- Приклад 2: The animal didn't cross the street because **it** was too tired.\n",
        "\n",
        "Слово \"воно\" не існує незалежно від відповідного іменника."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbeb0015-0e4c-41f5-b4e0-eb32b5396df0",
      "metadata": {
        "id": "cbeb0015-0e4c-41f5-b4e0-eb32b5396df0"
      },
      "source": [
        "### Проблема 2: RNNs працюють послідовно і тому повільно. Потрібна архітектура, яку легко паралелізувати\n",
        "![rnn](https://miro.medium.com/v2/resize:fit:1400/format:webp/0*oY-GwnsZDEaHdVyf.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e164fa71-8ee5-4528-9463-fa5f97bda24b",
      "metadata": {
        "id": "e164fa71-8ee5-4528-9463-fa5f97bda24b"
      },
      "source": [
        "## Transformer architecture(s)\n",
        "\n",
        "Basic components:\n",
        "- **attention** blocks\n",
        "- fully connected layers\n",
        "\n",
        "![components](http://jalammar.github.io/images/t/Transformer_encoder.png)\n",
        "\n",
        "See a very good explanation here: http://jalammar.github.io/illustrated-transformer/\n",
        "\n",
        "### Attention (self-attention)\n",
        "![att](https://i.imgur.com/PHWQnbX.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "058cf181-bd8f-471c-b995-5485f6d0e071",
      "metadata": {
        "id": "058cf181-bd8f-471c-b995-5485f6d0e071"
      },
      "source": [
        "### Attention: key + value + query\n",
        "\n",
        "The key/value/query concept is analogous to retrieval systems. For example, when you search for videos on Youtube, the search engine will map your query (text in the search bar) against a set of keys (video title, description, etc.) associated with candidate videos in their database, then present you the best matched videos (values).\n",
        "\n",
        "The attention operation can be thought of as a retrieval process as well.\n",
        "\n",
        "Source: https://stats.stackexchange.com/questions/421935/what-exactly-are-keys-queries-and-values-in-attention-mechanisms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf776e5b-0ee0-479d-a196-005af20af4e9",
      "metadata": {
        "id": "bf776e5b-0ee0-479d-a196-005af20af4e9"
      },
      "outputs": [],
      "source": [
        "# Attention\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def softmax(x):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum()\n",
        "\n",
        "def attention(query, key, value):\n",
        "    \"\"\"Compute attention scores.\"\"\"\n",
        "    # Calculate dot product of query and key\n",
        "    scores = np.dot(query, key)\n",
        "\n",
        "    # Apply softmax to obtain attention weights\n",
        "    attention_weights = softmax(scores)\n",
        "\n",
        "    # Weighted sum of value vectors\n",
        "    output = np.dot(attention_weights, value)\n",
        "\n",
        "    return output, attention_weights\n",
        "\n",
        "# Example input\n",
        "query = np.array([0.5, 0.2, 0.1])  # Query vector\n",
        "key = np.array([[0.8, 0.2, 0.1],    # Key vectors\n",
        "                 [0.3, 0.6, 0.4],\n",
        "                 [0.2, 0.5, 0.7]])\n",
        "value = np.array([[3, 0],            # Value vectors\n",
        "                  [5, 6],\n",
        "                  [7, 8]])\n",
        "\n",
        "# Compute attention\n",
        "output, attention_weights = attention(query, key.T, value)\n",
        "\n",
        "print(\"Attention output:\", output)\n",
        "print(\"Attention weights:\", attention_weights)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce38f34c-74ef-4522-b8c7-906a344e13e8",
      "metadata": {
        "id": "ce38f34c-74ef-4522-b8c7-906a344e13e8"
      },
      "outputs": [],
      "source": [
        "query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35884f48-a099-451b-b442-7aec484566cd",
      "metadata": {
        "id": "35884f48-a099-451b-b442-7aec484566cd"
      },
      "outputs": [],
      "source": [
        "key.T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddb2a7c7-d225-46ce-b626-56be78bf3757",
      "metadata": {
        "id": "ddb2a7c7-d225-46ce-b626-56be78bf3757"
      },
      "outputs": [],
      "source": [
        "scores = np.dot(query, key.T)\n",
        "scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d22e1ff3-18b4-4cac-a547-543b8945a7c4",
      "metadata": {
        "id": "d22e1ff3-18b4-4cac-a547-543b8945a7c4"
      },
      "outputs": [],
      "source": [
        "attention_weights = softmax(scores)\n",
        "attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4c1dc51-4b61-4f7f-9e10-d3f3d120b90f",
      "metadata": {
        "id": "d4c1dc51-4b61-4f7f-9e10-d3f3d120b90f"
      },
      "outputs": [],
      "source": [
        "output = np.dot(attention_weights, value)\n",
        "output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4859f181-c1ed-4e94-9d26-1f96b4bebda6",
      "metadata": {
        "id": "4859f181-c1ed-4e94-9d26-1f96b4bebda6"
      },
      "outputs": [],
      "source": [
        "# Calculate attention weights\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Sentence\n",
        "sentence = \"The animal didn't cross the street because it was too tired.\"\n",
        "\n",
        "# Tokenize the sentence\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "# Convert tokens to token IDs\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(\"Token IDs:\", token_ids)\n",
        "\n",
        "# Assuming query, key, and value are all token embeddings obtained from BERT\n",
        "# For simplicity, let's assume query, key, and value are all randomly generated for each token\n",
        "embeddings_dim = 128\n",
        "query = np.random.rand(len(token_ids), embeddings_dim)\n",
        "key = np.random.rand(len(token_ids), embeddings_dim)\n",
        "value = np.random.rand(len(token_ids), embeddings_dim)\n",
        "\n",
        "# Compute attention scores\n",
        "outputs = []\n",
        "attention_weights = []\n",
        "for i in range(len(token_ids)):\n",
        "    output, attn_weights = attention(query[i], key.T, value)\n",
        "    outputs.append(output)\n",
        "    attention_weights.append(attn_weights)\n",
        "\n",
        "print(\"Attention output for each token:\", outputs)\n",
        "print(\"Attention weights for each token:\", attention_weights)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "823ad617-69b6-4748-b40c-7c4532aee098",
      "metadata": {
        "id": "823ad617-69b6-4748-b40c-7c4532aee098"
      },
      "outputs": [],
      "source": [
        "len(tokens) # tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "875962d9-422a-4c74-8d92-5c91ebff145e",
      "metadata": {
        "id": "875962d9-422a-4c74-8d92-5c91ebff145e"
      },
      "outputs": [],
      "source": [
        "len(outputs[0])  # embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddf9010a-52ed-4048-b2df-7b586ca0c204",
      "metadata": {
        "id": "ddf9010a-52ed-4048-b2df-7b586ca0c204"
      },
      "outputs": [],
      "source": [
        "outputs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5c8cd9c-1606-4936-b1c0-5f921d5d1dce",
      "metadata": {
        "id": "b5c8cd9c-1606-4936-b1c0-5f921d5d1dce"
      },
      "outputs": [],
      "source": [
        "len(attention_weights[0]) # attention_weights of each word to each word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "398f3ce8-aaae-4b06-bc04-c1f6eeb0745e",
      "metadata": {
        "id": "398f3ce8-aaae-4b06-bc04-c1f6eeb0745e"
      },
      "outputs": [],
      "source": [
        "attention_weights[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fe04222-ea7e-48d2-9f36-3166b3b6decf",
      "metadata": {
        "id": "7fe04222-ea7e-48d2-9f36-3166b3b6decf"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to plot attention weights\n",
        "def plot_attention_weights(attention_weights, tokens):\n",
        "    fig, ax = plt.subplots()\n",
        "    im = ax.imshow(attention_weights, cmap='viridis')\n",
        "\n",
        "    # Set ticks for tokens\n",
        "    ax.set_xticks(np.arange(len(tokens)))\n",
        "    ax.set_yticks(np.arange(len(tokens)))\n",
        "    # Label ticks with the respective list entries\n",
        "    ax.set_xticklabels(tokens)\n",
        "    ax.set_yticklabels(tokens)\n",
        "\n",
        "    # Rotate the tick labels and set their alignment.\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
        "\n",
        "    # Loop over data dimensions and create text annotations.\n",
        "    for i in range(len(tokens)):\n",
        "        for j in range(len(tokens)):\n",
        "            text = ax.text(j, i, round(attention_weights[i, j], 2), ha=\"center\", va=\"center\", color=\"black\")\n",
        "\n",
        "    ax.set_title(\"Attention Weights\")\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Convert attention weights list to numpy array\n",
        "attention_weights_array = np.array(attention_weights)\n",
        "\n",
        "# Plot attention weights\n",
        "plot_attention_weights(attention_weights_array, tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3b40e7b-512b-4d12-aba1-63f44835cbbd",
      "metadata": {
        "id": "f3b40e7b-512b-4d12-aba1-63f44835cbbd"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from keras.layers import Input, Dense, Dropout, LayerNormalization, MultiHeadAttention, Embedding\n",
        "from keras.models import Model\n",
        "\n",
        "class TransformerEncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(num_heads, d_model)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            Dense(dff, activation='relu'),\n",
        "            Dense(d_model)\n",
        "        ])\n",
        "\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = Dropout(rate)\n",
        "        self.dropout2 = Dropout(rate)\n",
        "\n",
        "    def call(self, x, training=None):\n",
        "        attn_output = self.mha(x, x, x)  # (batch_size, input_seq_len, d_model)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        return out2\n",
        "\n",
        "class TransformerEncoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = Embedding(input_vocab_size, d_model)\n",
        "        self.pos_encoding = self.positional_encoding(maximum_position_encoding, self.d_model)\n",
        "\n",
        "        self.enc_layers = [TransformerEncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
        "\n",
        "        self.dropout = Dropout(rate)\n",
        "\n",
        "    def call(self, x, training=None):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        # Adding embedding and position encoding.\n",
        "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'num_layers': self.num_layers,\n",
        "            'd_model': self.d_model,\n",
        "            'num_heads': self.num_heads,\n",
        "            'dff': self.dff,\n",
        "            'input_vocab_size': self.input_vocab_size,\n",
        "            'maximum_position_encoding': self.maximum_position_encoding,\n",
        "            'rate': self.rate,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def positional_encoding(self, position, d_model):\n",
        "        angle_rads = self.get_angles(np.arange(position)[:, np.newaxis], np.arange(d_model)[np.newaxis, :], d_model)\n",
        "\n",
        "        # apply sin to even indices in the array; 2i\n",
        "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "        # apply cos to odd indices in the array; 2i+1\n",
        "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "        pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "    def get_angles(self, pos, i, d_model):\n",
        "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "        return pos * angle_rates\n",
        "\n",
        "# Example usage\n",
        "num_layers = 6\n",
        "d_model = 512\n",
        "num_heads = 8\n",
        "dff = 2048\n",
        "input_vocab_size = 10000\n",
        "maximum_position_encoding = 10000\n",
        "rate = 0.1\n",
        "\n",
        "inputs = Input(shape=(None,))\n",
        "encoder = TransformerEncoder(num_layers, d_model, num_heads, dff,\n",
        "                             input_vocab_size, maximum_position_encoding, rate)\n",
        "\n",
        "outputs = encoder(inputs)\n",
        "model = Model(inputs, outputs)\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3e01715-5301-4d40-a086-6c89662d1dc9",
      "metadata": {
        "id": "c3e01715-5301-4d40-a086-6c89662d1dc9"
      },
      "source": [
        "## Pretrained models and Finetuning\n",
        "\n",
        "https://huggingface.co/learn/nlp-course/en/chapter1/4\n",
        "\n",
        "![arch](https://textbook.edu.goit.global/python/data-science-remaster/v1/img/module-11/example5.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "967ab15c-708c-4a79-95dd-26fd131292cc",
      "metadata": {
        "id": "967ab15c-708c-4a79-95dd-26fd131292cc"
      },
      "outputs": [],
      "source": [
        "# use DistilBERT out of the box\n",
        "import torch\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load IMDb dataset\n",
        "dataset = load_dataset('imdb')\n",
        "\n",
        "# Load DistilBERT tokenizer and model\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "pretrained_model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# Example text for prediction\n",
        "example_text = \"This movie was great! I loved the acting and the storyline.\"\n",
        "\n",
        "# Tokenize input text\n",
        "inputs = tokenizer(example_text, return_tensors='pt')\n",
        "inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28122444-53dd-42ec-992a-fd6f15299e34",
      "metadata": {
        "id": "28122444-53dd-42ec-992a-fd6f15299e34"
      },
      "outputs": [],
      "source": [
        "# Make prediction\n",
        "outputs_pretrained = pretrained_model(**inputs)\n",
        "predicted_label_pretrained = torch.argmax(outputs_pretrained.logits).item()\n",
        "\n",
        "# Map predicted label to sentiment\n",
        "sentiment_pretrained = 'positive' if predicted_label_pretrained == 1 else 'negative'\n",
        "\n",
        "print(\"Predicted sentiment:\", sentiment_pretrained)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "657b867b-54f9-4b36-a85b-f8e22192176c",
      "metadata": {
        "id": "657b867b-54f9-4b36-a85b-f8e22192176c"
      },
      "outputs": [],
      "source": [
        "# Finetune DistilBERT\n",
        "\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"  # вимикаємо Weights & Biases\n",
        "\n",
        "import torch\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import load_dataset\n",
        "\n",
        "# 1. Завантаження IMDb датасету\n",
        "dataset = load_dataset('imdb')\n",
        "\n",
        "# 2. Перетворення в списки\n",
        "texts = list(dataset['train']['text'])\n",
        "labels = list(dataset['train']['label'])\n",
        "\n",
        "# 3. Train/Test Split\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    texts, labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 4. Завантаження токенайзера та моделі\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\n",
        "    'distilbert-base-uncased',\n",
        "    num_labels=2\n",
        ")\n",
        "\n",
        "# 5. Токенізація\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
        "\n",
        "# 6. Torch Dataset\n",
        "class IMDbDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = IMDbDataset(train_encodings, train_labels)\n",
        "test_dataset = IMDbDataset(test_encodings, test_labels)\n",
        "\n",
        "# 7. Метрика\n",
        "def compute_metrics(p):\n",
        "    preds = p.predictions.argmax(-1)\n",
        "    return {\"accuracy\": accuracy_score(p.label_ids, preds)}\n",
        "\n",
        "# 8. Параметри тренування\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=1,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=100,\n",
        "    save_strategy=\"epoch\"\n",
        ")\n",
        "\n",
        "# 9. Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# 10. Навчання\n",
        "trainer.train()\n",
        "\n",
        "# 11. Оцінка\n",
        "print(trainer.evaluate())\n",
        "\n",
        "# 12. Збереження моделі\n",
        "model.save_pretrained(\"./distilbert-imdb\")\n",
        "tokenizer.save_pretrained(\"./distilbert-imdb\")\n",
        "\n",
        "# 13. Передбачення на своєму тексті\n",
        "example_text = \"This movie was absolutely fantastic! Great acting and story.\"\n",
        "inputs = tokenizer(example_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    probs = torch.softmax(outputs.logits, dim=-1)\n",
        "\n",
        "print(\"Probabilities:\", probs)\n",
        "print(\"Predicted class:\", torch.argmax(probs).item())  # 1 = позитив, 0 = негатив\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a171a91-74e0-4821-8e1f-e371b5fd0686",
      "metadata": {
        "id": "9a171a91-74e0-4821-8e1f-e371b5fd0686"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ba2aa28-a01d-43f8-aa4e-b6233f0f78da",
      "metadata": {
        "id": "5ba2aa28-a01d-43f8-aa4e-b6233f0f78da"
      },
      "outputs": [],
      "source": [
        "# Make prediction\n",
        "# Example text for prediction\n",
        "example_text = \"This movie was great! I loved the acting and the storyline.\"\n",
        "\n",
        "# Tokenize input text\n",
        "inputs = tokenizer(example_text, return_tensors='pt')\n",
        "# if you work on a mac:\n",
        "#inputs = tokenizer(example_text, return_tensors='pt').to(\"mps\")\n",
        "outputs = model(**inputs)\n",
        "predicted_label = torch.argmax(outputs.logits).item()\n",
        "\n",
        "# Map predicted label to sentiment\n",
        "sentiment = 'positive' if predicted_label == 1 else 'negative'\n",
        "\n",
        "print(\"Predicted sentiment:\", sentiment)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "# Запакуємо папку з моделлю в zip\n",
        "shutil.make_archive(\"distilbert-imdb\", 'zip', \"./distilbert-imdb\")\n",
        "\n",
        "# Завантажимо zip-файл на комп’ютер\n",
        "files.download(\"distilbert-imdb.zip\")\n"
      ],
      "metadata": {
        "id": "wcmbsmI_DhF9"
      },
      "id": "wcmbsmI_DhF9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b64a7cfa-9f43-42e5-b02e-a3d87b440460",
      "metadata": {
        "id": "b64a7cfa-9f43-42e5-b02e-a3d87b440460"
      },
      "source": [
        "## From BERT to LLMs\n",
        "\n",
        "https://arxiv.org/abs/2402.06196v2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db487ac1-98de-4478-b26c-417e51ebdf33",
      "metadata": {
        "id": "db487ac1-98de-4478-b26c-417e51ebdf33"
      },
      "source": [
        "# More resources"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2dde4afa-b1a1-4f07-a8bf-b94b1bf9b320",
      "metadata": {
        "id": "2dde4afa-b1a1-4f07-a8bf-b94b1bf9b320"
      },
      "source": [
        "- Huggingface: https://huggingface.co (pretrained models, datasets, libraries)\n",
        "- Tutorials from huggingface: https://huggingface.co/learn/nlp-course/chapter1/1\n",
        "- Spacy: https://spacy.io/usage/embeddings-transformers#static-vectors\n",
        "- For work with LLMs: Prompt engineering guide - https://www.promptingguide.ai"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}