# üìö NLP Summary Task: README.md

## üß† –ó–∞–≤–¥–∞–Ω–Ω—è

–°—Ñ–æ—Ä–º—É–π—Ç–µ –∫–æ—Ä–æ—Ç–∫–µ —Ç–µ–∫—Å—Ç–æ–≤–µ —Ä–µ–∑—é–º–µ –∞–Ω–≥–ª–æ–º–æ–≤–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç—É –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é **–±—ñ–±–ª—ñ–æ—Ç–µ–∫ NLTK —Ç–∞ SpaCy**.

üìå **–í—Ö—ñ–¥–Ω—ñ –¥–∞–Ω—ñ:** —ñ—Å—Ç–æ—Ä–∏—á–Ω–∏–π –æ–ø–∏—Å –∫–æ—Å–º—ñ—á–Ω–æ–≥–æ —á–æ–≤–Ω–∏–∫–∞ *Discovery*.  
üìå **–†–µ–∑—É–ª—å—Ç–∞—Ç:** —Ç–µ–∫—Å—Ç–æ–≤–µ —Ä–µ–∑—é–º–µ, —è–∫–µ –æ—Ö–æ–ø–ª—é—î –æ—Å–Ω–æ–≤–Ω—É —Å—É—Ç—å –≤–∏—Ö—ñ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç—É.

---

## üîß –Ü–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∏

### 1. üêç [NLTK (Natural Language Toolkit)](https://www.nltk.org/)

- **–ú–æ–∂–ª–∏–≤–æ—Å—Ç—ñ**:
  - –¢–æ–∫–µ–Ω—ñ–∑–∞—Ü—ñ—è —Ä–µ—á–µ–Ω—å —ñ —Å–ª—ñ–≤
  - –°—Ç–æ–ø-—Å–ª–æ–≤–∞
  - –ß–∞—Å—Ç–æ—Ç–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ —Å–ª—ñ–≤
  - –°—Ç–µ–º—ñ–Ω–≥ / –ª–µ–º–∞—Ç–∏–∑–∞—Ü—ñ—è (–æ–±–º–µ–∂–µ–Ω–æ)
  - –ü–æ–±—É–¥–æ–≤–∞ –ø—Ä–æ—Å—Ç–æ–≥–æ extractive summary
  
- **–û—Å–Ω–æ–≤–Ω—ñ —ñ–º–ø–æ—Ä—Ç–∏**:
```python
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize

nltk.download('punkt')
nltk.download('stopwords')
text = """..."""
sentences = sent_tokenize(text)
words = word_tokenize(text)
filtered = [w for w in words if w.lower() not in stopwords.words('english')]
```

### 2. ‚öôÔ∏è SpaCy
- **–ú–æ–∂–ª–∏–≤–æ—Å—Ç—ñ:**
    - –ü–æ—Ç—É–∂–Ω–∞ –ª–µ–º–∞—Ç–∏–∑–∞—Ü—ñ—è

    - –¢–æ–∫–µ–Ω—ñ–∑–∞—Ü—ñ—è —Ç–∞ POS-—Ç–µ–≥–∏

    - –°–∏–Ω—Ç–∞–∫—Å–∏—á–Ω–∏–π –∞–Ω–∞–ª—ñ–∑

    - –ü–æ—à—É–∫ —ñ–º–µ–Ω–æ–≤–∞–Ω–∏—Ö —Å—É—Ç–Ω–æ—Å—Ç–µ–π

    - –§–æ—Ä–º—É–≤–∞–Ω–Ω—è —è–∫—ñ—Å–Ω–∏—Ö embeddings

- **–û—Å–Ω–æ–≤–Ω—ñ —ñ–º–ø–æ—Ä—Ç–∏**:
```python
import spacy
nlp = spacy.load("en_core_web_sm")
doc = nlp(text)
tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]
```

### üîÅ –ê–ª–≥–æ—Ä–∏—Ç–º extractive summarization (—Å–ø—ñ–ª—å–Ω–æ –∑ heapq)

```python
from heapq import nlargest

# –ß–∞—Å—Ç–æ—Ç–Ω–∏–π —Å–ª–æ–≤–Ω–∏–∫ —Å–ª—ñ–≤
word_frequencies = {}
for word in doc:
    if word.text.lower() not in stopwords.words('english') and word.text.isalpha():
        word_frequencies[word.text.lower()] = word_frequencies.get(word.text.lower(), 0) + 1

# –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è
max_freq = max(word_frequencies.values())
for word in word_frequencies:
    word_frequencies[word] /= max_freq

# –û—Ü—ñ–Ω—é–≤–∞–Ω–Ω—è —Ä–µ—á–µ–Ω—å
sentence_scores = {}
for sent in doc.sents:
    for word in sent:
        if word.text.lower() in word_frequencies:
            sentence_scores[sent] = sentence_scores.get(sent, 0) + word_frequencies[word.text.lower()]

# –†–µ–∑—é–º–µ: top N —Ä–µ—á–µ–Ω—å
summary_sentences = nlargest(3, sentence_scores, key=sentence_scores.get)
summary = " ".join([sent.text for sent in summary_sentences])
print(summary)
```

### üìö –î–æ–¥–∞—Ç–∫–æ–≤—ñ –º–∞—Ç–µ—Ä—ñ–∞–ª–∏

- üîó **NLTK**:
    [–û—Ñ—ñ—Ü—ñ–π–Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—è](https://www.nltk.org/)

     [NLTK Book](https://www.nltk.org/book/)

      –°—Ç–æ–ø-—Å–ª–æ–≤–∞, —Å—Ç–µ–º—ñ–Ω–≥: nltk.corpus.stopwords, PorterStemmer

- üîó **SpaCy**:
  [–û—Ñ—ñ—Ü—ñ–π–Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—è](https://spacy.io/usage)

    [–û–Ω–ª–∞–π–Ω –¥–µ–º–æ-–ø–∞—Ä—Å–µ—Ä](https://demos.explosion.ai/displacy)

–ú–æ–¥–µ–ª—ñ: en_core_web_sm, en_core_web_md, xx_ent_wiki_sm

- üìñ **–Ü–Ω—à–µ —Ü—ñ–∫–∞–≤–µ**:
üìò [Sumy](https://github.com/miso-belica/sumy) ‚Äî —â–µ –æ–¥–Ω–∞ –±—ñ–±–ª—ñ–æ—Ç–µ–∫–∞ –¥–ª—è extractive summarization

üìò [Gensim TextRank](https://radimrehurek.com/gensim/)

üß† [BERTSum, Pegasus (Google)] ‚Äî SOTA —É abstractive summarization (–ø–æ—Ç—Ä–µ–±—É—é—Ç—å GPU)