{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SiracencoSerghei/DataScienceHW/blob/main/HW3/hw3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Jb6SJ4JaRPcB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Write the linear regression hypothesis function in vector form;\n",
        "напишіть функцію гіпотези лінійної регресії у векторному вигляді;\n",
        "---"
      ],
      "metadata": {
        "id": "gTg6gB1fRQRJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$h(\\overrightarrow{X}) = \\overrightarrow{w} \\cdot  \\overrightarrow{X}$$\n",
        "or\n",
        "$$\n",
        "h(\\mathbf{X}) = \\mathbf{w}^T \\mathbf{X}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "bd6E0a93Yixf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def hypothesis(w, X):\n",
        "  \"\"\"\n",
        "    Calculates the predicted values (hypothesis) for linear regression.\n",
        "\n",
        "    Parameters:\n",
        "    w (array-like): A 1D array or vector of weights (model coefficients)\n",
        "                    including the intercept term.\n",
        "    X (array-like): A 1D or 2D array representing the feature matrix.\n",
        "                    Each row corresponds to a data point, and each column to a feature.\n",
        "\n",
        "    Returns:\n",
        "    float or array-like: The predicted value(s) based on the input features and weights.\n",
        "                         If X is 1D, a single value is returned.\n",
        "                         If X is 2D, a 1D array of predictions is returned.\n",
        "\n",
        "    Explanation:\n",
        "    - The function computes the dot product of the weights (w) and the features (X),\n",
        "      which represents the linear combination of the inputs weighted by the model coefficients.\n",
        "    - This corresponds to the formula h(X) = w_0 + w_1*x_1 + w_2*x_2 + ... + w_n*x_n,\n",
        "      where w_0 is the intercept, and w_1 to w_n are the feature coefficients.\n",
        "    \"\"\"\n",
        "  return np.dot(w, X)"
      ],
      "metadata": {
        "id": "L0kPjILXRXdN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Create a function to calculate the loss function in vector form;\n",
        "створіть функцію для обчислення функції втрат у векторному вигляді;\n",
        "---"
      ],
      "metadata": {
        "id": "bmLdZEWPTYTx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "Loss = \\frac{1}{2n} \\| Xw - y \\|^2\n",
        "$$\n",
        "or\n",
        "$$\n",
        "Loss = \\frac{1}{2n} \\sum_{i=1}^{n} (h(X_i) - y_i)^2\n",
        "$$\n",
        "\n"
      ],
      "metadata": {
        "id": "EP9-PogfgTrW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_func(features, target, weights):\n",
        "  \"\"\"\n",
        "  Calculates the mean squared error (MSE) loss/cost/error for linear regression.\n",
        "\n",
        "  Parameters:\n",
        "  target (array-like): A 1D array of target values (dependent variable, y).\n",
        "  features (array-like): A 2D array where each row represents a data point and each column represents a feature (independent variables, X).\n",
        "  weights (array-like): A 1D array of model weights (coefficients), including the intercept term.\n",
        "\n",
        "  Returns:\n",
        "  float: The computed loss value (MSE) for the given weights and data.\n",
        "\n",
        "  Explanation:\n",
        "  - The loss function measures how well the model's predictions match the target values.\n",
        "  - The formula for the loss is: L(w) = (1 / (2 * n)) * Σ(h(X_i) - y_i)^2\n",
        "    where:\n",
        "      - n is the number of data points,\n",
        "      - h(X_i) = X @ w is the predicted value for the i-th data point,\n",
        "      - y_i is the actual target value for the i-th data point.\n",
        "  \"\"\"\n",
        "  n = len(target)\n",
        "  h = hypothesis(features, weights)\n",
        "  loss = (1/(2*n)) * np.sum((h - target)**2)\n",
        "  return loss"
      ],
      "metadata": {
        "id": "4vaYSqcGTtS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Implement one step of gradient descent;\n",
        "реалізуйте один крок градієнтного спуску;\n",
        "---"
      ],
      "metadata": {
        "id": "XeDdzUM30R3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent_step(X, y, w, learning_rate=.001):\n",
        "    \"\"\"\n",
        "    Performs a single step of gradient descent to update the model's weights.\n",
        "\n",
        "    Parameters:\n",
        "    w (array-like): The current weights of the model.\n",
        "    X (array-like): The feature matrix.\n",
        "    y (array-like): The target values.\n",
        "    learning_rate (float): The learning rate (step size) for the gradient descent.\n",
        "\n",
        "    Returns:\n",
        "    array-like: The updated weights after one gradient descent step.\n",
        "    \"\"\"\n",
        "    n = len(y)\n",
        "    # Обчислюємо похибку\n",
        "    h = hypothesis(w, X)\n",
        "    error = h - y\n",
        "    # Calculate the gradient of the loss function\n",
        "    gradient = (1 / n) * np.dot(X.T, error)\n",
        "\n",
        "    # Update the weights using the gradient and learning rate\n",
        "    w -= learning_rate * gradient\n",
        "    return w"
      ],
      "metadata": {
        "id": "ebdmQe9C269A"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, y, w, num_iterations=1, learning_rate=0.001):\n",
        "    \"\"\"\n",
        "    Implements gradient descent for linear regression.\n",
        "\n",
        "    Parameters:\n",
        "    X (array-like): Feature matrix (independent variables), dimensions (m, n).\n",
        "    y (array-like): Target vector (dependent variable), dimensions (m, 1).\n",
        "    w (array-like): Weight vector (coefficients), dimensions (n, 1).\n",
        "    num_iterations (int): Number of iterations to update the weights.\n",
        "    learning_rate (float): Step size for gradient descent.\n",
        "\n",
        "    Returns:\n",
        "    tuple:\n",
        "        - w: The final weight vector after optimization.\n",
        "        - loss_history: An array storing the loss at each iteration.\n",
        "    \"\"\"\n",
        "    # Initialize an array to store the loss at each iteration\n",
        "    loss_history = np.zeros(num_iterations)\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "        # Perform a single gradient descent step to update weights\n",
        "        w = gradient_descent_step(X, y, w, learning_rate)\n",
        "\n",
        "        # Compute the loss (Mean Squared Error) for the current weights\n",
        "        loss_history[i] = loss_func(X, y, w)\n",
        "\n",
        "    return w, loss_history\n",
        "\n"
      ],
      "metadata": {
        "id": "h1mvHhJzBUPB"
      },
      "execution_count": 6,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}